# -*- coding: utf-8 -*-
"""oversampling_technique_applied.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nVF5bpNDlwsT0-O-f3kpP08iBpepTEay

# Importing the dependencies
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from wordcloud import WordCloud
from collections import Counter
from sklearn.preprocessing import LabelEncoder
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import numpy as np
from datasets import load_dataset, Dataset, DatasetDict
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

from google.colab import drive
drive.mount('/content/drive')

"""# Loading the dataset"""

latest_df = pd.read_csv("/content/drive/MyDrive/Hate_speech_detection_system/data_set/first_term_final_training_data.csv")

# Shape of the dataframe
latest_df.shape

# Unique category present in the data
latest_df['label'].unique()

# Count of each unique category present in the data
label_counts = latest_df['label'].value_counts()
label_counts

"""# Basic data distribution"""

# Set style
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# 1. Class Distribution
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
# fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))
# fig, ax1 = plt.subplots(figsize=(8, 6))

# Bar plot
label_counts.plot(kind='bar', ax=ax1, color=['#0088FE', '#00C49F', '#FFBB28', '#FF8042'])
ax1.set_title('Class Distribution', fontsize=14, fontweight='bold')
ax1.set_ylabel('Count')
ax1.tick_params(axis='x', rotation=45)

# Pie chart
ax2.pie(label_counts.values, labels=label_counts.index, autopct='%1.1f%%',
        colors=['#0088FE', '#00C49F', '#FFBB28', '#FF8042'])
ax2.set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')

# Text length analysis (you'll need to calculate this)
latest_df['text_length'] = latest_df['text'].str.len()
sns.boxplot(data=latest_df, x='label', y='text_length', ax=ax3)
ax3.set_title('Text Length Distribution by Class', fontsize=14, fontweight='bold')
ax3.tick_params(axis='x', rotation=45)

# Token count analysis
# Token count analysis with NaN handling
latest_df['token_count'] = latest_df['text'].fillna('').str.split().apply(len)
sns.boxplot(data=latest_df, x='label', y='token_count', ax=ax4)
ax4.set_title('Token Count Distribution by Class', fontsize=14, fontweight='bold')
ax4.tick_params(axis='x', rotation=45)

# Imbalance visualization
# imbalance_ratio = label_counts.max() / label_counts.min()
# ax4.bar(['Imbalance Ratio'], [imbalance_ratio], color='coral')
# ax4.set_title(f'Class Imbalance Ratio: {imbalance_ratio:.2f}:1', fontsize=14, fontweight='bold')
# ax4.set_ylabel('Ratio')

plt.tight_layout()
plt.show()

"""# Applying oversampling to balance the dataset"""

# Set target count for each class
target_count = latest_df['label'].value_counts().max()

# Create a list to store balanced data
balanced_df = []

# For each class, duplicate data as needed
for label in latest_df['label'].unique():
    class_df = latest_df[latest_df['label'] == label]
    if len(class_df) < target_count:
        class_df = class_df.sample(target_count, replace=True, random_state=42)  # Oversample
    else:
        class_df = class_df.sample(target_count, random_state=42)  # Undersample if needed

# Count original labels
label_counts = latest_df['label'].value_counts()
print(label_counts)

# Set target count for each class
# target_count = label_counts.max()
target_count = 5000

# Create a list to store balanced data
balanced_df_list = []

# For each class, duplicate data as needed
for label in latest_df['label'].unique():
    class_df = latest_df[latest_df['label'] == label]
    if len(class_df) < target_count:
        class_df = class_df.sample(target_count, replace=True, random_state=42)  # Oversample
    else:
        class_df = class_df.sample(target_count, random_state=42)  # Undersample if needed
    balanced_df_list.append(class_df)

# Combine all balanced class data into one DataFrame
df_balanced = pd.concat(balanced_df_list).reset_index(drop=True)

# Now you can safely access 'label'
df_balanced['label'].value_counts()

"""# Encode string labels to integers"""

label_encoder = LabelEncoder()
df_balanced['label'] = label_encoder.fit_transform(df_balanced['label'])

# Save label mapping for future use
label2id = {label: idx for idx, label in enumerate(label_encoder.classes_)}
id2label = {idx: label for label, idx in label2id.items()}

label2id

"""# Training metrics visualization tool"""

!pip install wandb
# 3d3e348d51aca106577d4475eaae33ca9a770d37

!wandb login

print(f"CUDA Available: {torch.cuda.is_available()}")
# print(f"GPU: {torch.cuda.get_device_name(0)}")import torch
print(f"CUDA Available: {torch.cuda.is_available()}")
# print(f"GPU: {torch.cuda.get_device_name(0)}")

"""# Common utils"""

# 8. Define compute_metrics
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')
    acc = accuracy_score(labels, predictions)
    return {
        'accuracy': acc,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

# Preprocessing function
def preprocess_function(examples, tokenizer):
    texts = [str(t) if t is not None else "" for t in examples["text"]]
    return tokenizer(
        texts,
        padding=True,          # Pad to longest in batch
        truncation=True,       # Truncate to model's max length
        max_length=512,        # Optional: set your max length
        return_tensors="pt"
    )

def train_transformer_model(
    model_name: str,
    df,
    num_labels: int,
    preprocess_function,
    output_dir: str,
    compute_metrics=None,
    test_size: float = 0.1,
    batch_size: int = 20,
    num_train_epochs: int = 10,
    learning_rate: float = 1e-5,
    weight_decay: float = 0.1,
    warmup_steps: int = 500,
    fp16: bool = True,
    gradient_accumulation_steps: int = 2,
    logging_steps: int = 100,
    eval_steps: int = 200,
    save_steps: int = 1200,
    save_total_limit: int = 3,
):
    # Load tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)
    print(f"Model '{model_name}' loaded successfully!")

    # Convert dataframe to Dataset object and split
    dataset = Dataset.from_pandas(df)
    dataset = dataset.train_test_split(test_size=test_size)

    # Tokenize the dataset
    # tokenized_dataset = dataset.map(preprocess_function, batched=True)
    tokenized_dataset = dataset.map(
    lambda x: preprocess_function(x, tokenizer=tokenizer),
    batched=True
    )

    # Inspect one sample (optional)
    print("Sample tokenized data:", tokenized_dataset["train"][0])

    # Define training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        eval_strategy="steps",
        eval_steps=eval_steps,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        num_train_epochs=num_train_epochs,
        learning_rate=learning_rate,
        weight_decay=weight_decay,
        warmup_steps=warmup_steps,
        logging_steps=logging_steps,
        save_steps=save_steps,
        save_total_limit=save_total_limit,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        fp16=fp16,
        dataloader_drop_last=True,
        gradient_accumulation_steps=gradient_accumulation_steps
    )

    # Prepare collator
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    # Setup trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset["train"],
        eval_dataset=tokenized_dataset["test"],
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    # Train the model
    print("Starting fine-tuning...")
    train_result = trainer.train()
    print("Fine-tuning complete!")

    return trainer, train_result, tokenizer, tokenized_dataset

def evaluate_and_plot_results(trainer, train_result, tokenized_test_set, class_names=None):
    # 1. Save model and tokenizer (optional)
    # trainer.save_model("your_save_path")
    # trainer.tokenizer.save_pretrained("your_save_path")

    # 2. Evaluate metrics
    metrics = trainer.evaluate()
    print("Evaluation Metrics:", metrics)

    # 3. Extract training and evaluation losses
    logs = trainer.state.log_history
    train_loss_vals = [log["loss"] for log in logs if "loss" in log]
    eval_loss_vals = [log["eval_loss"] for log in logs if "eval_loss" in log]

    epochs_train = list(range(1, len(train_loss_vals) + 1))
    epochs_eval = list(range(1, len(eval_loss_vals) + 1))

    # 4. Plot losses
    plt.figure(figsize=(10, 6))
    plt.plot(epochs_train, train_loss_vals, label="Training Loss")
    plt.plot(epochs_eval, eval_loss_vals, label="Evaluation Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Training and Evaluation Loss Over Time")
    plt.legend()
    plt.grid(True)
    plt.show()

    # 5. Get predictions
    predictions = trainer.predict(tokenized_test_set)
    y_pred = np.argmax(predictions.predictions, axis=1)
    y_true = predictions.label_ids

    # 6. Plot Confusion Matrix
    if class_names is None:
        class_names = list(set(y_true))  # default class names

    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
    disp.plot(cmap=plt.cm.Blues, xticks_rotation=45)
    plt.title("Confusion Matrix")
    plt.grid(False)
    plt.show()

    return metrics, y_true, y_pred

"""# Finetuning Indic Bert"""

# Call the function with your inputs
trainer, train_result, tokenizer, tokenized_dataset = train_transformer_model(
    model_name="ai4bharat/indic-bert",
    df=df_balanced,
    num_labels=4,
    preprocess_function=preprocess_function,
    output_dir="/content/drive/MyDrive/Hate_speech_detection_system/over_sample_data_train_models/indic_bert/training_checkpoints",
    compute_metrics=compute_metrics,  # your custom metric function
    num_train_epochs= 10
)

# 12. Save model and tokenizer
trainer.save_model("/content/drive/MyDrive/Hate_speech_detection_system/over_sample_data_train_models/indic_bert/models/model")
tokenizer.save_pretrained("/content/drive/MyDrive/Hate_speech_detection_system/over_sample_data_train_models/indic_bert/models/tokenizer")

"""##### Evaluate and plot metrices"""

class_names = ["bias", "hate_speech", "misinformation", "normal"]

metrics, y_true, y_pred = evaluate_and_plot_results(
    trainer=trainer,
    train_result=train_result,
    tokenized_test_set=tokenized_dataset["test"],
    class_names=class_names
)

"""# Finetuning mBert"""

# Call the function with your inputs
trainer, train_result, tokenizer, tokenized_dataset = train_transformer_model(
    model_name="bert-base-multilingual-cased",
    df=df_balanced,
    num_labels=4,
    preprocess_function=preprocess_function,
    output_dir="/content/drive/MyDrive/Hate_speech_detection_system/over_sample_data_train_models/mBert/training_checkpoints",
    compute_metrics=compute_metrics,  # your custom metric function
    num_train_epochs= 10
)

# 12. Save model and tokenizer
trainer.save_model("/content/drive/MyDrive/Hate_speech_detection_system/over_sample_data_train_models/mBert/models/model")
tokenizer.save_pretrained("/content/drive/MyDrive/Hate_speech_detection_system/over_sample_data_train_models/mBert/models/tokenizer")

class_names = ["bias", "hate_speech", "misinformation", "normal"]

metrics, y_true, y_pred = evaluate_and_plot_results(
    trainer=trainer,
    train_result=train_result,
    tokenized_test_set=tokenized_dataset["test"],
    class_names=class_names
)

"""# xlm-RoBERTa"""

# Call the function with your inputs
trainer, train_result, tokenizer, tokenized_dataset = train_transformer_model(
    model_name="xlm-roberta-base",
    df=df_balanced,
    num_labels=4,
    preprocess_function=preprocess_function,
    output_dir="/content/drive/MyDrive/Hate_speech_detection_system/over_sample_data_train_models/xlm_roberta/training_checkpoints",
    compute_metrics=compute_metrics,  # your custom metric function
    num_train_epochs= 10
)

# 12. Save model and tokenizer
trainer.save_model("/content/drive/MyDrive/Hate_speech_detection_system/over_sample_data_train_models/xlm_roberta/models/model")
tokenizer.save_pretrained("/content/drive/MyDrive/Hate_speech_detection_system/over_sample_data_train_models/xlm_roberta/models/tokenizer")

class_names = ["bias", "hate_speech", "misinformation", "normal"]

metrics, y_true, y_pred = evaluate_and_plot_results(
    trainer=trainer,
    train_result=train_result,
    tokenized_test_set=tokenized_dataset["test"],
    class_names=class_names
)



